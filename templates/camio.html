<!DOCTYPE html>
<head>
  
<style>
canvas {
    border: 1px solid black;
}
video {
    border: 1px solid black;
}
.err {
    color: red;
}
</style>
</head>
<body>
<link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
<script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>



<p class="err" id="vdErr"></p>
</div>
<div id="contentarea">
    <button id="startup" disabled="true" onclick="startup()">start</button><br>
    <video id="video">Your browser does not support the video tag.</video>
    <canvas id="canvasOutput0"></canvas>
<   <canvas id="canvasOutput"></canvas>
</div>

<script async src="{{ url_for('static', filename='opencv.js') }}" id="opencvjs"></script>

<script >

let gestureRecognizer;
let createGestureRecognizer;
let runningMode = "IMAGE";
let enableWebcamButton;
let webcamRunning = false;
const videoHeight = "480px";
const videoWidth = "640px";
// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.

const video1 = document.getElementById("video");
const canvasElement = document.getElementById("canvasOutput");
const canvasCtx = canvasElement.getContext("2d");

let lastVideoTime = -1;
let results = undefined;
async function predictWebcam() {
    const webcamElement = document.getElementById("video");
    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
    }
    let nowInMs = Date.now();
    if (video1.currentTime !== lastVideoTime) {
        lastVideoTime = video1.currentTime;
        results = gestureRecognizer.recognizeForVideo(video1, nowInMs);
    }
    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    canvasElement.style.height = videoHeight;
    webcamElement.style.height = videoHeight;
    canvasElement.style.width = videoWidth;
    webcamElement.style.width = videoWidth;
    if (results.landmarks) {
        for (const landmarks of results.landmarks) {
            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
                color: "#00FF00",
                lineWidth: 5
            });
            drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });
        }
    }
    // canvasCtx.restore();
}

// In this case, We set width 320, and the height will be computed based on the input stream.
let width = 640;
let height = 0;

// whether streaming video from the camera.
let streaming = false;

// Some HTML elements we need to configure.
let video = null;
let start = null;
let stream = null;

let loopIndex = 0;

let cap = null;
let inputImage = null;
let markerImage = null;
let dictionary = null;
let parameter = null;
let markerIds = null;
let markerCorners = null;
let RgbImage = null;

function get_aruco(){
    inputImage = new cv.Mat(height, width, cv.CV_8UC4);
    markerImage = new cv.Mat();
    dictionary = new cv.aruco_Dictionary(cv.DICT_4X4_250);

    markerIds = new cv.Mat();
    markerCorners  = new cv.MatVector();
    RgbImage = new cv.Mat();
    cap = new cv.VideoCapture("video");
    loopIndex = setInterval(
        // function(){
        //     cap.read(inputImage);
        //     cv.cvtColor(inputImage, RgbImage, cv.COLOR_RGBA2RGB, 0);
        //     cv.detectMarkers(RgbImage, dictionary, markerCorners, markerIds);
        //     if (markerIds.rows > 0) {
        //         cv.drawDetectedMarkers(RgbImage, markerCorners, markerIds);
        //     }
        //     cv.imshow("canvasOutput", RgbImage);
        // }
        one_time_step
        , 100);
}

function one_time_step(){
    predictWebcam();

    cap.read(inputImage);
    cv.cvtColor(inputImage, RgbImage, cv.COLOR_RGBA2RGB, 0);
    cv.detectMarkers(RgbImage, dictionary, markerCorners, markerIds);
    if (markerIds.rows > 0) {
        cv.drawDetectedMarkers(RgbImage, markerCorners, markerIds);
    }
    cv.imshow("canvasOutput0", RgbImage);

}

function initVideo(ev){
    if (!streaming) {
        height = video.videoHeight / (video.videoWidth/width);
        video.setAttribute("width", width);
        video.setAttribute("height", height);
        streaming = true;
    }
    playVideo();
}

function startup() {
    video = document.getElementById("video");
    start = document.getElementById("startup");

    navigator.mediaDevices.getUserMedia({ video: {facingMode: "environment"}, audio: false })
        .then(function(s) {
            stream = s;
            video.srcObject = stream;
            video.play();
        })
        .catch(function(err) {
            console.log("An error occured! " + err);
    });

    video.addEventListener("canplay", initVideo, false);
}

function playVideo() {
    if (!streaming) {
        console.warn("Please startup your webcam");
        return;
    }
    try {
        get_aruco();
        document.getElementById("vdErr").innerHTML = " ";
    } catch(err) {
        document.getElementById("vdErr").innerHTML = err;
    }
    start.disabled = true;
}

function onReady() {
    document.getElementById("startup").disabled = false;
}
if (typeof cv !== 'undefined') {
    onReady();
} else {
    document.getElementById("opencvjs").onload = onReady;
}
</script>

<script id="rendered-js" type="module">
  import { GestureRecognizer, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
  const createGestureRecognizer = async () => {
        const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
        gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
            baseOptions: {
                modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                delegate: "GPU"
            },
            runningMode: runningMode
        });
    };
    createGestureRecognizer();

    window.gestureRecognizer = gestureRecognizer;
    window.runningMode = runningMode;
</script>


</body>