<!DOCTYPE html>
<head>
  
<style>
canvas {
    border: 1px solid black;
}
video {
    border: 1px solid black;
}
.err {
    color: red;
}

</style>

<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
<script defer src="https://pyscript.net/latest/pyscript.js"></script>

</head>
<body>
<link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
<script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>

<p class="err" id="vdErr"></p>
</div>
<div id="contentarea">
    <button id="startup"  onclick="startup()">start</button>
    <button id="stop"  onclick="stopCamera()">stop</button><br>
    <video id="video"  > </video>
    <canvas id="canvasOutput"></canvas>
    <canvas id="map_canvas"></canvas>
</div>

<py-config type="json">
    {
        "packages": ["numpy", "matplotlib","opencv-python"]
    }
</py-config>
 <py-script src="./static/homography.py"></py-script>  

<py-script>
import cv2
from cv2 import aruco
import numpy as np
import base64
from js import createObject
from pyodide.ffi import create_proxy
createObject(create_proxy(globals()), "pyodideGlobals")
</py-script>

<script >
function createObject(object, variableName){
    //Bind a variable whose name is the string variableName
    // to the object called 'object'
    let execString = variableName + " = object"
    console.log("Running '" + execString + "'");
    eval(execString)
}

dataURL = "map_canvas.toDataURL()"; 
const map_canvas = document.getElementById('map_canvas');
const map_ctx = map_canvas.getContext('2d');
const map_img = new Image();
map_img.src = 'https://huiyingshen.github.io/flask_opencv_js/market_tmap.png';
map_img.crossOrigin = "Anonymous";
map_img.onload = function() {
    map_canvas.width = map_img.width;
    map_canvas.height = map_img.height;
    map_ctx.drawImage(map_img, 0, 0, map_canvas.width, map_canvas.height); // Drawing the image on the canvas at position (0,0) and scaling it to fill the canvas
    dataURL = map_canvas.toDataURL().substring(21); 
};



let width = 640;
let height = 480;

let gestureRecognizer;
let createGestureRecognizer;
let runningMode = "IMAGE";
let enableWebcamButton;
let webcamRunning = false;

const videoHeight = height.toString() + "px";
const videoWidth = width.toString() + "px";
// Before we can use HandLandmarker class we must wait for it to finish
// loading. Machine Learning models can be large and take a moment to
// get everything needed to run.

const video1 = document.getElementById("video");
const canvasElement = document.getElementById("canvasOutput");
const canvasCtx = canvasElement.getContext("2d");

let lastVideoTime = -1;
let results = undefined;
async function predictWebcam() {
    const webcamElement = document.getElementById("video");
    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
        runningMode = "VIDEO";
        await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
    }
    let nowInMs = Date.now();
    if (video1.currentTime !== lastVideoTime) {
        lastVideoTime = video1.currentTime;
        results = gestureRecognizer.recognizeForVideo(video1, nowInMs);
    }
    // canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    canvasElement.style.height = videoHeight;
    webcamElement.style.height = videoHeight;
    canvasElement.style.width = videoWidth;
    webcamElement.style.width = videoWidth;
    if (results.landmarks) {
        for (const landmarks of results.landmarks) {
            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
                color: "#00FF00",
                lineWidth: 1
            });
            // drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 1 });
            canvasCtx.beginPath();
            let x = landmarks[8]["x"]*width;
            let y = landmarks[8]["y"]*height;
            console.log("tip: x,y = ", x,y);
            canvasCtx.ellipse(x, y, 5, 5, 0, 0, 2 * Math.PI);
            canvasCtx.stroke();
        }
    }
    // canvasCtx.restore();
    if (webcamRunning === true) {
        window.requestAnimationFrame(predictWebcam);
    }
}

// In this case, We set width 320, and the height will be computed based on the input stream.


// whether streaming video from the camera.
let streaming = false;

// Some HTML elements we need to configure.
let video = null;
let start = null;
let stream = null;





function initVideo(ev){
    if (!streaming) {
        height = video.videoHeight / (video.videoWidth/width);
        video.setAttribute("width", width);
        video.setAttribute("height", height);
        streaming = true;
    }
    playVideo();
}

function startup() {
    video = document.getElementById("video");
    start = document.getElementById("startup");

    navigator.mediaDevices.getUserMedia({ 
            video: {
                facingMode: "environment",
                width: { min: 640, ideal: 1280, max: 1920 },
                height: {min: 480, ideal: 720, max: 1080 }
            }, 
            audio: false 
        })
        .then(function(s) {
            stream = s;
            video.srcObject = stream;
            video.play();
        })
        .catch(function(err) {
            console.log("An error occured! " + err);
    });

    video.addEventListener("canplay", initVideo, false);
}
function stopCamera() {
    
    document.getElementById("canvasOutput").getContext("2d").clearRect(0, 0, width, height);
    video.pause();
    video.srcObject = null;
    stream.getVideoTracks()[0].stop();
    start.disabled = false;
    video.removeEventListener("canplay", initVideo);
}

function playVideo() {
    if (!streaming) {
        console.warn("Please startup your webcam");
        return;
    }
    webcamRunning = true;
    try {

    predictWebcam() 

        document.getElementById("vdErr").innerHTML = " ";
    } catch(err) {
        document.getElementById("vdErr").innerHTML = err;
    }
    start.disabled = true;
}



</script>

<script id="rendered-js" type="module">
  import { GestureRecognizer, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";
  const createGestureRecognizer = async () => {
        const vision = await FilesetResolver.forVisionTasks("https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm");
        gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
            baseOptions: {
                modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                delegate: "GPU"
            },
            runningMode: runningMode
        });
    };
    createGestureRecognizer();

    window.gestureRecognizer = gestureRecognizer;
    window.runningMode = runningMode;
</script>
</body>
